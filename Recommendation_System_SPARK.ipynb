{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "# spark = SparkSession.builder.master(\"local\").appName(\"ALS Model\").config(\"spark.executor.memory\", \"20G\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "        .appName(\"ALS Model\") \\\n",
    "        .config(\"spark.executor.memory\", \"14G\") \\\n",
    "        .config(\"spark.executor.cores\", \"8\") \\\n",
    "        .config(\"spark.driver.memory\",'13G')\\\n",
    "        .config(\"spark.driver.cores\", \"4\") \\\n",
    "        .config(\"spark.memory.fraction\", 0.8) \\\n",
    "        .config(\"spark.rpc.message.maxSize\",\"400\")\\\n",
    "        .config(\"spark.default.parallelism\", \"800\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"800\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Checkpoint Directory\n",
    "spark.sparkContext.setCheckpointDir('***********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true')\\\n",
    ".option('inferSchema','true')\\\n",
    ".csv('********.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "df = df.withColumnRenamed(\"Quantity Delivered (No UOM)\", \"Rating\") \\\n",
    "   .withColumnRenamed(\"CustomerID\",\"CustomerID\")\\\n",
    "   .withColumnRenamed(\"StockCode\",\"ItemID\")\\\n",
    "   .withColumnRenamed(\"Description\",\"Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of columns\n",
    "df = df.withColumn(\"CustomerID\", df[\"CustomerID\"].cast(IntegerType())) \\\n",
    "   .withColumn(\"ItemID\", df[\"ItemID\"].cast(IntegerType())) \\\n",
    "   .withColumn(\"Rating\",df[\"Rating\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 4 columns\n",
    "dataset = df.select('CustomerID','ItemID','Rating','Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Decription is not equal to %NW_Online%\n",
    "NW_Filter = dataset.filter(\"Description not like '%NW_Online%'\")\n",
    "NW_Filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "dataset_Filter = NW_Filter.filter(\"CustomerID is not NULL and ItemID is not NULL and Rating is not NULL\")\n",
    "# Select 3 columns without \"Description\"\n",
    "dataset = dataset_Filter.select('CustomerID','ItemID','Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 4 columns with \"Description\" for the result\n",
    "dataset_4Columns = dataset_Filter.select('CustomerID','ItemID','Rating','Description')\n",
    "dataset_4Columns.show(20,False)\n",
    "\n",
    "dataset.show()\n",
    "dataset.printSchema()\n",
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test sets\n",
    "Train, Test = dataset.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ALS model\n",
    "als = ALS(userCol= \"CustomerID\", itemCol= \"ItemID\", ratingCol= \"Rating\",\n",
    "          coldStartStrategy=\"drop\", nonnegative= True)\n",
    "# Tune model using ParamGridBuilder\n",
    "param_grid = ParamGridBuilder()\\\n",
    "               .addGrid(als.rank, [150])\\\n",
    "               .addGrid(als.maxIter, [50])\\\n",
    "               .addGrid(als.regParam, [0.1])\\\n",
    "               .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator as RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ALS model to training data\n",
    "model = cv.fit(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model from the tuning exercise using ParamGridBuilder\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Save the model\n",
    "best_model.save('********')\n",
    "\n",
    "# Load the data\n",
    "best_model = ALSModel.load('********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on data\n",
    "predictions = model.transform(Test)\n",
    "\n",
    "# Generate predicions and evaluate using RMSE\n",
    "predictions = best_model.transform(Test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE =\" + str(rmse))\n",
    "print(\"**Best Model**\")\n",
    "print(\"Rank:\"), best_model.rank\n",
    "print(\"Rank:\"), best_model._java_obj.parent().getMaxIter()\n",
    "print(\"Rank:\"), best_model._java_obj.parent().getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one user's previous Item\n",
    "Filter_user = dataset_4Columns.filter(\"CustomerID == ***** \")\n",
    "Filter_user.sort(\"Rating\",ascending=False).show(10000, False)\n",
    "dataset_4Columns.show()\n",
    "# Display the Result\n",
    "predictions.sort(\"CustomerID\",\"Rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one user's recommendation\n",
    "# Get the top 10 recommend for All users But hard to read..\n",
    "user_recs = best_model.recommendForAllUsers(20)\n",
    "user_recs = best_model.recommendForUsers(20)\n",
    "user_recs.show()\n",
    "# Save to csv\n",
    "user_recs.toPandas().to_csv('*****.csv')\n",
    "# Find one user's recommendations\n",
    "recs = user_recs.filter(\"CustomerID == *****\")\n",
    "recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After modeling, we get the final output for one customer && More readable!!\n",
    "def get_recs_for_user(recs, dataset_4Columns):\n",
    "    # Recs should be for a specific user.\n",
    "    recs = recs.select(\"CustomerID\",\"recommendations.ItemID\", \"recommendations.rating\")\n",
    "    Customers = recs.select(\"CustomerID\").toPandas().iloc[0, 0]\n",
    "    Items = recs.select(\"ItemID\").toPandas().iloc[0,0]\n",
    "    ratings = recs.select(\"rating\").toPandas().iloc[0, 0]\n",
    "    ratings_matrix = pd.DataFrame(Items, columns = [\"ItemID\"])\n",
    "    ratings_matrix[\"ratings\"] = ratings\n",
    "    ratings_matrix[\"CustomerID\"] = Customers\n",
    "    ratings_matrix_ps = spark.createDataFrame(ratings_matrix)\n",
    "    # Join two tables and get the \"Description\"\n",
    "    df1 = ratings_matrix_ps.alias('df1')\n",
    "    df2 = dataset_4Columns.alias('df2')\n",
    "    RS_datafram = df1.join(df2, df1.ItemID == df2.ItemID).select('df1.*', 'df2.Description')\n",
    "    return RS_datafram.distinct().sort(\"ratings\", ascending=False).show(20, False)\n",
    "# Apply to see the Result\n",
    "\n",
    "get_recs_for_user(recs, dataset_4Columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
